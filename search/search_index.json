{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NDCMS User Documentation [UNDER CONSTRUCTION] The page contains information pertaining to the Notre Dame High Energy Physics Research Group.","title":"HOME"},{"location":"#ndcms-user-documentation","text":"[UNDER CONSTRUCTION] The page contains information pertaining to the Notre Dame High Energy Physics Research Group.","title":"NDCMS User Documentation"},{"location":"apps/dv5/","text":"Placeholder for DV5 application page.","title":"DV5"},{"location":"apps/eft/","text":"Placeholder for EFT application page.","title":"EFT"},{"location":"resources/caml/","text":"CAML This page contains information regarding the NSF funded Cyberinfrastructure to Accelerate Machine Learning (CAML) computing resource. CAML provides GPU resources for accelerating machine learning to the research community both locally at the University of Notre Dame and nationally through the Open Science Grid (OSG). CAML physically hosts GPU resources suitable for accelerating the training of models from standard Deep Learning libraries. Configured for both interactive and batch access, CAML supports both small-scale explorations to large-scale discovery science. Access requirements In order for local users to access this resource, a CRC account and access to the Panasas (scratch) storage. Please, follow the links below for more information on how to obtain these: Obtaining a CRC account: https://docs.crc.nd.edu/new_user/obtain_account.html Panasas storage: https://docs.crc.nd.edu/infrastructure/storage.html?highlight=scratch365#panasas-scratch-storage Access through OSG can be done by signing up to OSG Connect: https://www.osgconnect.net Note only 20% of resources are allocated via this method at this point. Resource description The CAML GPU cluster is composed of: camlnd.crc.nd.edu (frontend / submit host) - 17 Dual 12-cores Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz, 192 GB of Memory, 400 GB Disk 4 GPUs NVIDIA Quadro RTX 6000 - 2 Dual 12-cores Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz, 192 GB of Memory, 400 GB Disk 4 GPUs NVIDIA Tesla V100-PCIE-32GB Using the resource on campus The university provides two different methods to access this resource: through a Jupyterhub service and via HTCondor. The former allows users to start interactive Jupyterhub notebook sessions on CAML GPU resources. The latter (HTCondor) is a batch system that provides a job queueing mechanism, allowing the scheduling of multiple jobs to this resource in parallel. Jupyterhub service Use this method if you need to access a single GPU resource interactively, through notebook sessions from your browser. Pre-requisites From a remote location (e.g. off campus, etc.), one must first log into the VPN. More information is available in the section off-campus-connect . Instructions After you get an account in camlnd.crc.nd.edu, you can click on the URL below in order to user Jupyterhub with CAML resources. This will start jupyter notebooks inside the GPU nodes for a specific amount of time. At present, access to your AFS Home area is allowed in read-only mode, use /scratch365/\\$USER to write files. https://camlnd.crc.nd.edu:9800/ Use your campus credentials to log in. Then, set the following parameters (note more demanding parameters may take longer to be scheduled, so please try to estimate the resources and time needed for your work as best as possible): GPUs: Number of GPUs for your application, up to 4. Default: 1 CPUs: Number of CPUs. Default: 1 Memory: Requested Memory (GB) Runtime: Maximum wall clock time (in minutes) allowed for your notebook to run. Max: 24 hours, Default:2 hours. Container: This will be the software environment needed for your application. Please, ask the CRC for help if you need an environment not listed here. Finally, you can either run python notebooks or work directly in the terminal. You can try the pytorch notebook example with the pytorch container. Note your starting directory will be located on /scratch365/\\$USER. Please, refer to the jupyter-notebook documentation for more information. HTCondor Submission In order to use CAML resources, we require defining singularity images on PATHs. E.g.: Via cvmfs. For example, the following Notre Dame image (available via CVMFS) supports TensorFlow, Keras and PyTorch: /cvmfs/singularity.opensciencegrid.org/notredamedulac/el7-tensorflow-pytorch:latest and it is built from: https://github.com/NDCMS/el7-tensorflow-gpu This section will show submission examples for different workflows, pointing to the specific Singularity images to handle the software environment dependencies. HTCondor submission file HTCondor needs a submission file, describing the resources needed, input files, log paths, container image location and executable to run. E.g.: executable = yourexecutable.sh # Note if you use a subdirectory like logs, you need to create it beforehand # E.g.: mkdir logs Log = logs/$(Cluster).log Output = logs/$(Cluster)-$(Process).out Error = logs/$(Cluster)-$(Process).err # Input files # You can transfer files like this: should_transfer_files = Yes transfer_input_files = /some/path/input_file # But if you are using /scratch365 and all your input files are present there, all worker nodes will have read/write access to it, so you can just use the scratch365 paths instead without transferring the input files and set the following instead: should_transfer_files = IF_NEEDED # Singularity image, for amber for example, it would be: #+SingularityImage = \"cvmfs/singularity.opensciencegrid.org/notredamedulac/amber:latest\" # Number of GPUs, Cpus and Memory to use request_gpus = 1 request_memory = 1 Gb request_cpus = 1 # Number of jobs to submit Queue 1 Please, refer to the HTCondor documentation for more details on how to create a submission file. Setting up the tutorial examples Once you log in into camlnd.crc.nd.edu, type: cd /scratch365/$USER git clone https://github.com/ND-CRC/ndgputests cd ndgputests Then, follow one of the examples below. Tensorflow example $ condor_submit submit_tensorflow.jdl Output example: - Executing python TensorFlow matrix multiplication example 2020-03-25 13:18:29.845113: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compil ed to use: AVX2 AVX512F FMA 2020-03-25 13:18:30.004982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.62 pciBusID: 0000:2f:00.0 totalMemory: 22.17GiB freeMemory: 22.00GiB 2020-03-25 13:18:30.005161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0 2020-03-25 13:18:30.570670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-03-25 13:18:30.570757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2020-03-25 13:18:30.570794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2020-03-25 13:18:30.570954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU :0 with 21320 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:2f:00.0, compute capability: 7.5) result of matrix multiplication =============================== [[ 1.0000000e+00 0.0000000e+00] [-4.7683716e-07 1.0000002e+00]] =============================== Pytorch example $ condor_submit submit_pytorch.jdl Output example: - Executing python torch matrix multiplication example --- Debug information --- - torch version: 1.4.0 - GPU information: -- CUDA Available: True -- Number of devices: 1 -- CUDA Device Name: Quadro RTX 6000 -- Current CUDA device: 0 ------ result of matrix multiplication =============================== tensor([[ 1.0000e+00, 0.0000e+00], [-4.7684e-07, 1.0000e+00]], device='cuda:0') Amber example $ condor_submit submit_amber.jdl Output example: - Entering scratch365 submit directory - Executing pmemd.cuda - Amber job is completed. - Exit code: 0 # From mdout |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Quadro RTX 6000 | CUDA Device Global Mem Size: 22698 MB | CUDA Device Num Multiprocessors: 72 | CUDA Device Core Freq: 1.62 GHz | |-------------------------------------------------------- JAX example: $ condor_submit submit_jax.jdl Output example: - Executing python JAX matrix multiplication example -----GPU devices information----- [GpuDevice(id=0)] GPU host id: 0 --------------------------------- Generate a random matrix [[ 1.3890220e+00 -3.2292119e-01 1.5543443e-01 ... 1.6672333e-01 1.0217550e+00 9.6981764e-02] [ 1.0637628e+00 -1.8089763e+00 -7.7909984e-02 ... 1.1778636e+00 -4.3357372e-01 -2.7877533e-01] [-4.4029754e-01 -3.2537547e-01 2.7817255e-01 ... 6.8317270e-01 -6.1108190e-01 -6.3071573e-01] ... [ 2.9218230e-01 -4.0055802e-01 -1.4978158e+00 ... 3.0673659e+00 -1.1350130e+00 4.0964666e-01] [ 2.7635786e-01 1.5621810e-01 2.2997444e-03 ... 6.8930797e-02 -4.0692501e-02 4.1683877e-01] [ 1.0231308e+00 -2.7423611e-01 -8.0369931e-01 ... 1.9415886e+00 1.0946993e+00 2.1876085e+00]] Multiply by its transpose: result of matrix multiplication ================================= [[2938.1716 17.38843 36.508224 ... 32.315964 51.31904 -34.432022] [ 17.38843 3031.179 41.194584 ... 47.24877 58.077858 -13.371615] [ 36.508224 41.194584 3000.4697 ... 8.109009 -42.50184 26.49511 ] ... [ 32.315964 47.24877 8.109009 ... 2916.339 34.381073 39.404526] [ 51.31904 58.077858 -42.50184 ... 34.381073 3032.2844 63.69183 ] [ -34.432022 -13.371615 26.49511 ... 39.404526 63.69183 3033.4866 ]] ] ] ] ] ]] ] ] ] ] ]] DeepSphere example $ condor_submit submit_deepsphere.jdl Part of output example: emory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:2f:00.0, compute capability: 7.5) 2020-05-18 14:09:22.115326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.62 pciBusID: 0000:2f:00.0 step 75 / 112 (epoch 8.00 / 12): learning_rate = 1.00e-01, training loss = 3.12e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.38e+00 CPU time: 10s, wall time: 11s step 90 / 112 (epoch 9.60 / 12): learning_rate = 1.00e-01, training loss = 5.68e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.41e+00 CPU time: 11s, wall time: 12s step 105 / 112 (epoch 11.20 / 12): learning_rate = 1.00e-01, training loss = 2.04e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.44e+00 CPU time: 12s, wall time: 13s step 112 / 112 (epoch 11.95 / 12): learning_rate = 1.00e-01, training loss = 1.00e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.44e+00 CPU time: 13s, wall time: 14s validation accuracy: best = 100.00, mean = 100.00","title":"CAML"},{"location":"resources/caml/#caml","text":"This page contains information regarding the NSF funded Cyberinfrastructure to Accelerate Machine Learning (CAML) computing resource. CAML provides GPU resources for accelerating machine learning to the research community both locally at the University of Notre Dame and nationally through the Open Science Grid (OSG). CAML physically hosts GPU resources suitable for accelerating the training of models from standard Deep Learning libraries. Configured for both interactive and batch access, CAML supports both small-scale explorations to large-scale discovery science.","title":"CAML"},{"location":"resources/caml/#access-requirements","text":"In order for local users to access this resource, a CRC account and access to the Panasas (scratch) storage. Please, follow the links below for more information on how to obtain these: Obtaining a CRC account: https://docs.crc.nd.edu/new_user/obtain_account.html Panasas storage: https://docs.crc.nd.edu/infrastructure/storage.html?highlight=scratch365#panasas-scratch-storage Access through OSG can be done by signing up to OSG Connect: https://www.osgconnect.net Note only 20% of resources are allocated via this method at this point.","title":"Access requirements"},{"location":"resources/caml/#resource-description","text":"The CAML GPU cluster is composed of: camlnd.crc.nd.edu (frontend / submit host) - 17 Dual 12-cores Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz, 192 GB of Memory, 400 GB Disk 4 GPUs NVIDIA Quadro RTX 6000 - 2 Dual 12-cores Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz, 192 GB of Memory, 400 GB Disk 4 GPUs NVIDIA Tesla V100-PCIE-32GB","title":"Resource description"},{"location":"resources/caml/#using-the-resource-on-campus","text":"The university provides two different methods to access this resource: through a Jupyterhub service and via HTCondor. The former allows users to start interactive Jupyterhub notebook sessions on CAML GPU resources. The latter (HTCondor) is a batch system that provides a job queueing mechanism, allowing the scheduling of multiple jobs to this resource in parallel.","title":"Using the resource on campus"},{"location":"resources/caml/#jupyterhub-service","text":"Use this method if you need to access a single GPU resource interactively, through notebook sessions from your browser.","title":"Jupyterhub service"},{"location":"resources/caml/#pre-requisites","text":"From a remote location (e.g. off campus, etc.), one must first log into the VPN. More information is available in the section off-campus-connect .","title":"Pre-requisites"},{"location":"resources/caml/#instructions","text":"After you get an account in camlnd.crc.nd.edu, you can click on the URL below in order to user Jupyterhub with CAML resources. This will start jupyter notebooks inside the GPU nodes for a specific amount of time. At present, access to your AFS Home area is allowed in read-only mode, use /scratch365/\\$USER to write files. https://camlnd.crc.nd.edu:9800/ Use your campus credentials to log in. Then, set the following parameters (note more demanding parameters may take longer to be scheduled, so please try to estimate the resources and time needed for your work as best as possible): GPUs: Number of GPUs for your application, up to 4. Default: 1 CPUs: Number of CPUs. Default: 1 Memory: Requested Memory (GB) Runtime: Maximum wall clock time (in minutes) allowed for your notebook to run. Max: 24 hours, Default:2 hours. Container: This will be the software environment needed for your application. Please, ask the CRC for help if you need an environment not listed here. Finally, you can either run python notebooks or work directly in the terminal. You can try the pytorch notebook example with the pytorch container. Note your starting directory will be located on /scratch365/\\$USER. Please, refer to the jupyter-notebook documentation for more information.","title":"Instructions"},{"location":"resources/caml/#htcondor-submission","text":"In order to use CAML resources, we require defining singularity images on PATHs. E.g.: Via cvmfs. For example, the following Notre Dame image (available via CVMFS) supports TensorFlow, Keras and PyTorch: /cvmfs/singularity.opensciencegrid.org/notredamedulac/el7-tensorflow-pytorch:latest and it is built from: https://github.com/NDCMS/el7-tensorflow-gpu This section will show submission examples for different workflows, pointing to the specific Singularity images to handle the software environment dependencies.","title":"HTCondor Submission"},{"location":"resources/caml/#htcondor-submission-file","text":"HTCondor needs a submission file, describing the resources needed, input files, log paths, container image location and executable to run. E.g.: executable = yourexecutable.sh # Note if you use a subdirectory like logs, you need to create it beforehand # E.g.: mkdir logs Log = logs/$(Cluster).log Output = logs/$(Cluster)-$(Process).out Error = logs/$(Cluster)-$(Process).err # Input files # You can transfer files like this: should_transfer_files = Yes transfer_input_files = /some/path/input_file # But if you are using /scratch365 and all your input files are present there, all worker nodes will have read/write access to it, so you can just use the scratch365 paths instead without transferring the input files and set the following instead: should_transfer_files = IF_NEEDED # Singularity image, for amber for example, it would be: #+SingularityImage = \"cvmfs/singularity.opensciencegrid.org/notredamedulac/amber:latest\" # Number of GPUs, Cpus and Memory to use request_gpus = 1 request_memory = 1 Gb request_cpus = 1 # Number of jobs to submit Queue 1 Please, refer to the HTCondor documentation for more details on how to create a submission file.","title":"HTCondor submission file"},{"location":"resources/caml/#setting-up-the-tutorial-examples","text":"Once you log in into camlnd.crc.nd.edu, type: cd /scratch365/$USER git clone https://github.com/ND-CRC/ndgputests cd ndgputests Then, follow one of the examples below.","title":"Setting up the tutorial examples"},{"location":"resources/caml/#tensorflow-example","text":"$ condor_submit submit_tensorflow.jdl Output example: - Executing python TensorFlow matrix multiplication example 2020-03-25 13:18:29.845113: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compil ed to use: AVX2 AVX512F FMA 2020-03-25 13:18:30.004982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.62 pciBusID: 0000:2f:00.0 totalMemory: 22.17GiB freeMemory: 22.00GiB 2020-03-25 13:18:30.005161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0 2020-03-25 13:18:30.570670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-03-25 13:18:30.570757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2020-03-25 13:18:30.570794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2020-03-25 13:18:30.570954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU :0 with 21320 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:2f:00.0, compute capability: 7.5) result of matrix multiplication =============================== [[ 1.0000000e+00 0.0000000e+00] [-4.7683716e-07 1.0000002e+00]] ===============================","title":"Tensorflow example"},{"location":"resources/caml/#pytorch-example","text":"$ condor_submit submit_pytorch.jdl Output example: - Executing python torch matrix multiplication example --- Debug information --- - torch version: 1.4.0 - GPU information: -- CUDA Available: True -- Number of devices: 1 -- CUDA Device Name: Quadro RTX 6000 -- Current CUDA device: 0 ------ result of matrix multiplication =============================== tensor([[ 1.0000e+00, 0.0000e+00], [-4.7684e-07, 1.0000e+00]], device='cuda:0')","title":"Pytorch example"},{"location":"resources/caml/#amber-example","text":"$ condor_submit submit_amber.jdl Output example: - Entering scratch365 submit directory - Executing pmemd.cuda - Amber job is completed. - Exit code: 0 # From mdout |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Quadro RTX 6000 | CUDA Device Global Mem Size: 22698 MB | CUDA Device Num Multiprocessors: 72 | CUDA Device Core Freq: 1.62 GHz | |--------------------------------------------------------","title":"Amber example"},{"location":"resources/caml/#jax-example","text":"$ condor_submit submit_jax.jdl Output example: - Executing python JAX matrix multiplication example -----GPU devices information----- [GpuDevice(id=0)] GPU host id: 0 --------------------------------- Generate a random matrix [[ 1.3890220e+00 -3.2292119e-01 1.5543443e-01 ... 1.6672333e-01 1.0217550e+00 9.6981764e-02] [ 1.0637628e+00 -1.8089763e+00 -7.7909984e-02 ... 1.1778636e+00 -4.3357372e-01 -2.7877533e-01] [-4.4029754e-01 -3.2537547e-01 2.7817255e-01 ... 6.8317270e-01 -6.1108190e-01 -6.3071573e-01] ... [ 2.9218230e-01 -4.0055802e-01 -1.4978158e+00 ... 3.0673659e+00 -1.1350130e+00 4.0964666e-01] [ 2.7635786e-01 1.5621810e-01 2.2997444e-03 ... 6.8930797e-02 -4.0692501e-02 4.1683877e-01] [ 1.0231308e+00 -2.7423611e-01 -8.0369931e-01 ... 1.9415886e+00 1.0946993e+00 2.1876085e+00]] Multiply by its transpose: result of matrix multiplication ================================= [[2938.1716 17.38843 36.508224 ... 32.315964 51.31904 -34.432022] [ 17.38843 3031.179 41.194584 ... 47.24877 58.077858 -13.371615] [ 36.508224 41.194584 3000.4697 ... 8.109009 -42.50184 26.49511 ] ... [ 32.315964 47.24877 8.109009 ... 2916.339 34.381073 39.404526] [ 51.31904 58.077858 -42.50184 ... 34.381073 3032.2844 63.69183 ] [ -34.432022 -13.371615 26.49511 ... 39.404526 63.69183 3033.4866 ]] ] ] ] ] ]] ] ] ] ] ]]","title":"JAX example:"},{"location":"resources/caml/#deepsphere-example","text":"$ condor_submit submit_deepsphere.jdl Part of output example: emory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:2f:00.0, compute capability: 7.5) 2020-05-18 14:09:22.115326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.62 pciBusID: 0000:2f:00.0 step 75 / 112 (epoch 8.00 / 12): learning_rate = 1.00e-01, training loss = 3.12e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.38e+00 CPU time: 10s, wall time: 11s step 90 / 112 (epoch 9.60 / 12): learning_rate = 1.00e-01, training loss = 5.68e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.41e+00 CPU time: 11s, wall time: 12s step 105 / 112 (epoch 11.20 / 12): learning_rate = 1.00e-01, training loss = 2.04e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.44e+00 CPU time: 12s, wall time: 13s step 112 / 112 (epoch 11.95 / 12): learning_rate = 1.00e-01, training loss = 1.00e-03 validation accuracy: 100.00 (50 / 50), f1 (weighted): 100.00, loss: 1.44e+00 CPU time: 13s, wall time: 14s validation accuracy: best = 100.00, mean = 100.00","title":"DeepSphere example"},{"location":"resources/ndcms/","text":"NDCMS The page contains information pertaining to the Notre Dame High Energy Physics Research Group . Below you will find navigation links to all content within this page. Page Navigation [!NOTE] All of this documentation is specific to our NDCMS T3 cluster. It is written and maintained by the users of that cluster, which includes you. Please help us keep this up-to-date. The most important way in which you can do this is to send e-mail to NDT3@listserv.nd.edu if you try any of the documentation here and find it's out of date. Not on the mailing list? Contact klannon@nd.edu to get added. Logging In and Setting up your Software Environment From inside the ND network (being physically on campus or via VPN) or at CERN, one can log directly into the interactive head node: ssh -Y *username*@earth.crc.nd.edu Alternatively, if that doesn't work, try: /usr/bin/ssh -Y -l username@earth.crc.nd.edu From a remote location (e.g. off campus, etc.), one must first log into the VPN. More information is available in the section off-campus-connect . crcfe01.crc.nd.edu crcfe02.crc.nd.edu Tip: Don't forget the -Y option for both ssh commands if you want any X windows to show up on your screen (i.e. opening TBrowsers, histograms, etc.) For more comfort, you can create the file \".ssh/config\" on your local computer and insert an entry like Host earth, crcfe01, crcfe02 HostName %h.crc.nd.edu User *your_username* ForwardX11 yes ForwardX11Trusted yes ControlMaster auto ControlPath ~/.ssh/control-%r@%h:%p The last two lines are optional. They set up ssh to allow a second session to \"tunnel\" through the first one. As long as you have one ssh session open, further connections to earth do not require you to enter your password again. This should only be done for Notre Dame, as normally Kerberos authentication and ssh keys work better. With this setup, you may type just \"ssh earth\" to log into earth, and likewise for crcfe01 and crcfe02. Setting up environment [!NOTE] Only the earth.crc.nd.edu machine has the software required to use CMSSW, CRAB, etc. First login or when experiencing trouble with Condor: The first time you log into earth, set the AFS permissions of a directory to use for Condor as follows: Your home directory needs to be readable by anyone on campus because Condor doesn't run the jobs under your username. In your home directory, do the following: cd ~ fs sa . nd_campus rl fs sa . system:authuser rl If you have any subdirectories which you plan to use for Condor, you'll need to add read permissions for them as well. If you have a large number of subdirectories to modify see [[#If You Forgot to Set Your AFS Permissions at First Login|here]]. For the directory where CRAB is going to write outputs, you need to give write permission to any user on campus. Be careful doing this. You don't want to do this for directories where someone else could make a mistake and delete important files (like your home directory). Assuming your condor directory is called \"my condor_directory\", then you should do the following: cd my_condor_directory/ fs sa . nd_campus rlidwk fs sa . system:administrators rlidwka fs sa . system:authuser rlidwk You'll have to configure Condor so that any output files are stored in the specific directory to which you've granted write permissions. This will avoid possible problems in which condor does not have the correct permissions to launch jobs from your AFS space. [!WARNING] If you didn't do this the first time you logged in or created your Condor directory, this is fixable. See here <if-you-forgot> Add your proxy DN and Notre Dame username (your username on earth.crc.nd.edu) 1) Please add your proxy DN and ND username in the google doc below: https://docs.google.com/document/d/1piHU3tvAdPEXis-evx1mzOD8YwyQ59SbgeLUZR4U1cE/edit 2) Let us know that you've added your DN to the shared google doc file, please email ijohnso1@nd.edu and CC khurtado@nd.edu [!NOTE] If you would like to switch to a more modern shell, such as bash, you need to contact the CRC to have your shell changed in their database.** If You Forgot to Set Your AFS Permissions at First Login You can go back and set your AFS permissions recursively after the fact using the \"find\" command carefully: Starting in your home directory or one of your subdirectories, you can do the following to set read permissions for all subdirectories below. find . -type d -exec fs sa {} nd_campus rl \\; find . -type d -exec fs sa {} system:authuser rl \\; For the Condor directory and any of its subdirectories, you can use the following to set permissions correctly: cd my_condor_directory find . -type d -exec fs sa {} nd_campus rlidwk \\; find . -type d -exec fs sa {} system:administrators rlidwka \\; find . -type d -exec fs sa {} system:authuser rlidwk \\; PhEDEx Back to Top of the Page <ndcms> . The CMS experiment is managing hundreds petabytes of data recorded by the detector and simulated physics events. Data are transferred to distributed CMS sites for storage, processing and analysis. PhEDEx (Physics Experiment Data Export) maintains the knowledge of data replicas locations and manages data transfers between the sites. Requesting a PhEDEx Transfer to the Notre Dame Tier 3 Site (US_T3_NotreDame) use DAS https://cmsweb.cern.ch/das/] to find the dataset you're interested in transferring In the search field, use this format to search for your favorite dataset: dataset dataset=/Tau/Run2011A-PromptReco-v4/AOD Click on \"Subscribe to PhEDEx\" Request a transfer to the US_T3_NotreDame site Under \"Destinations\" check the box next to T3_US_NotreDame At the bottom of the page, click \"Submit Request\", and click \"confirm\" on the following page. You will receive an email notification with a link to the status of your request. You will receive a confirmation email when your request is approved by the site admin. Monitoring Tier 3 components and Jobs Back to Top of the Page <ndcms> . Below are a collection of links that can be used to try to monitor the health of the Tier 3 components. If you see unexpected behavior--fewer than normal running jobs, slower job I/O, failing file transfers, etc.--please check here for indications of what might be wrong. Note: Unless otherwise noted, the links below only work for those on campus or connected with a VPN. HTCondor Monitoring To check the general status of HTCondor within the CRC, you can view the HTCondor utilization matrix , for more information on HTCondor itself view the HTCondor <condor> page. The official documentation for conodr command line utilities can be found here . HTCondor Job Monitoring To view the status of all jobs in the condor queue (or for a specific user), use the following command: condor_q username Note: if you have submitted jobs from outside ND, condor maps your username to \"uscms01\". If you have jobs in the \"Idle\" state, you can ask condor why they are not running, using the following command: condor_q -better {jobID} This command will give you a list of all the reasons a job can be rejected, and tell you how many of the 600 nodes are rejecting your job for that reason. It is possible to manually abort your jobs using the following command: condor_rm (jobID or username) However, it is recommended to cancel the jobs via CRAB, because CRAB will often get confused if its jobs suddenly disappear. To abort jobs using CRAB, use: crab -kill (job_range or \"all\") The memory restriction per node on the condor queue is set to 1 GB by default. This is measured against the virtual memory of the process, meaning once a job's image size (virtual memory) becomes larger than 1 GB it could be idled indefinitely. Many CMSSW jobs tend to accumulate much more virtual memory than physical memory (up to 2x in some cases). This causes jobs to be idled when they don't necessarily need to be. You can change the default 1 GB memory restriction through the command condor_qedit. If you choose to change this value, try to be reasonably sure that your jobs won't be using more than 1 GB of real memory as this could cause problems for the machine the job is running on (including possibly crashing it). In order to change the memory limit, do the following: 1) find the job requirements string for one of your jobs: > condor_q -l <condor_job_id> | grep Requirements 2) copy the string in quotes that comes after \"Requirements = \" in the output. The requirements string will have an element like ( ( TARGET.Memory * 1024 ) >= ImageSize ) Since we can't edit the value of 'TARGET.Memory', we'll have to change the proportionality factor...from 1024 to, say, 2048. 3) reset the Requirements string for a job or group of jobs, using the old requirements string with the new proportionality factor. > condor_qedit <user/condor_job_id> Requirements <new_req_string> Note that since the default string contains double quotation marks (\"\"), you'll need to surround your \\<new_req_string> with single qoutes (''). In addition, if you copy and paste the output from step one, you'll need to remove the space in \"> =\" to have instead \">=\". Remember that 'condor_qedit' will work on a specific job ID (xxx.y), a job cluster ID (xxx), or a username. Storage Back to Top of the Page <ndcms> . Users have access to a variety of storage locations. The key is knowing the tradeoffs of each, so you can identify what's the right resource for you to use. If you don't have a directory in any of the spaces listed, ask for help at ndt3-list@nd.edu . Home Area Each user has a home directory on /afs/crc.nd.edu/user with 100GB of personal disk space that is backed up nightly. This is where you should keep software that you're developing, papers that you're writing, your thesis draft, etc. Basically, use this for anything that you would be very sad to have to recreate if it were accidentally deleted or lost due to hardware failure. To check the quota use fs lq (short for \u201cfileservice listquota\u201d) Scratch Space Also users get 500GB of non-backed up space in /scratch365/<username> and non-backed up space for small files in /store/smallfiles . There is no quota on /store/smallfiles but the total space is 80 TB and must be shared by all users. In general, this storage is useful to use for temporary files of intermediate sizes. If you need reasonable access performance for multiple jobs to the files (e.g. you're going to run more than ~100 jobs reading or writing files in the batch system) then don't use /store/smallfiles as the performance degrades severely. In that case, either use /scratch365 or /hadoop/store/user (see below). Accessing the scratch space from grid jobs Grid jobs running at ND (e.g: via CMS Connect) need to prepend /cms to the /scratch365 and /store/smallfiles directories ( /cms/scratch365/<username> and /cms/store/smallfiles ) due to standard requirements in the CMS Global Pool for singularity-enabled sites for non-hadoop storage spaces to be visible, but they point to the same storage location. Hadoop Space Hadoop is mounted at /hadoop/store/user . The Hadoop file system (hdfs) is a different sort of files system than most. Hadoop breaks your data up into blocks of ~128 MB and scatters two copies of each block across multiple physical disks. It does this for two reasons: The replication makes the system more resilient against hardware failures and it also provides better performance when many different jobs are reading or writing to the system. Like /store/smallfiles , Hadoop doesn't have per user quotas, and there is a lot of space available (at the time of this writing, 644 TB of raw space, but remember that every TB you store takes up ~2 TB of space because of replication). Hadoop is also the file system that is accessible with CMS/grid tools like gfal and XRootD. You should use Hadoop whenever you have very large datasets, when you need to access your data using gfal or XRootD, or when you will be accessing your data with many parallel running jobs (anything more than 100). There are some caveats: * Hadoop doesn't handle very small files well. If you write large numbers of files with sizes on the order of MB, don't use Hadoop. For files in that size range, use /store/smallfiles or /scratch365 * Hadoop doesn't provide posix access directly. This means that normally you can't use commands like ''ls'' or ''cp''. We use something called FUSE to provide posix access to Hadoop, but FUSE can be broken if you try to read too much data too quickly. So, when running many batch jobs, its better to access the data directly using hdfs commands, or to use a tool, like XRootD or Lobster that does this for you. If you're running jobs on data in /hadoop and they are getting stuck or having Input/Output errors, you've probably crashed FUSE on some of the nodes. If this happens, ask for help [ mailto:ndt3-list@nd.edu ndt3-list@nd.edu ]. * ROOT cannot write directly into /hadoop/store/user . If your job is producing ROOT output, write it first to local disk (every worker node has local disk for this purpose) and then at the end of the job, copy the output to /hadoop/store/user (possibly using gfal to avoid problems with FUSE!). Again, if you have questions, ask on ndt3-list@nd.edu . Lobster Working Storage Lobster doesn't do well working out of your AFS home directory. When you run Lobster jobs, you should tell Lobster to make your working area in /tmpscratch/users/<username> . Space is limited and there are no user quotas, so monitor carefully and clean up old files. We reserve the right to clean out this space if someone is using too much and not playing nice with others! [!NOTE] To Add you proxy DN and Notre Dame username, go to: https://wiki.crc.nd.edu/w/index.php/NDCMS_SettingUpEnvironment#Add_you_proxy_DN_and_Notre_Dame_username_.28your_username_on_earth.crc.nd.edu.29 Storage space for PhEDEx All CMS data are stored using the /store convention, Therefore we only need to map: /+store/(.*) Translation rules for PFN to LFN (Physical File Name to Logical File Name): /hadoop/store ==> /store To see how much space is available in the hadoop /store area, you can type the following from earth: hadoop fs -df -h /store","title":"NDCMS"},{"location":"resources/ndcms/#ndcms","text":"The page contains information pertaining to the Notre Dame High Energy Physics Research Group . Below you will find navigation links to all content within this page. Page Navigation [!NOTE] All of this documentation is specific to our NDCMS T3 cluster. It is written and maintained by the users of that cluster, which includes you. Please help us keep this up-to-date. The most important way in which you can do this is to send e-mail to NDT3@listserv.nd.edu if you try any of the documentation here and find it's out of date. Not on the mailing list? Contact klannon@nd.edu to get added.","title":"NDCMS"},{"location":"resources/ndcms/#logging-in-and-setting-up-your-software-environment","text":"From inside the ND network (being physically on campus or via VPN) or at CERN, one can log directly into the interactive head node: ssh -Y *username*@earth.crc.nd.edu Alternatively, if that doesn't work, try: /usr/bin/ssh -Y -l username@earth.crc.nd.edu From a remote location (e.g. off campus, etc.), one must first log into the VPN. More information is available in the section off-campus-connect . crcfe01.crc.nd.edu crcfe02.crc.nd.edu Tip: Don't forget the -Y option for both ssh commands if you want any X windows to show up on your screen (i.e. opening TBrowsers, histograms, etc.) For more comfort, you can create the file \".ssh/config\" on your local computer and insert an entry like Host earth, crcfe01, crcfe02 HostName %h.crc.nd.edu User *your_username* ForwardX11 yes ForwardX11Trusted yes ControlMaster auto ControlPath ~/.ssh/control-%r@%h:%p The last two lines are optional. They set up ssh to allow a second session to \"tunnel\" through the first one. As long as you have one ssh session open, further connections to earth do not require you to enter your password again. This should only be done for Notre Dame, as normally Kerberos authentication and ssh keys work better. With this setup, you may type just \"ssh earth\" to log into earth, and likewise for crcfe01 and crcfe02.","title":"Logging In and Setting up your Software Environment"},{"location":"resources/ndcms/#setting-up-environment","text":"[!NOTE] Only the earth.crc.nd.edu machine has the software required to use CMSSW, CRAB, etc. First login or when experiencing trouble with Condor: The first time you log into earth, set the AFS permissions of a directory to use for Condor as follows: Your home directory needs to be readable by anyone on campus because Condor doesn't run the jobs under your username. In your home directory, do the following: cd ~ fs sa . nd_campus rl fs sa . system:authuser rl If you have any subdirectories which you plan to use for Condor, you'll need to add read permissions for them as well. If you have a large number of subdirectories to modify see [[#If You Forgot to Set Your AFS Permissions at First Login|here]]. For the directory where CRAB is going to write outputs, you need to give write permission to any user on campus. Be careful doing this. You don't want to do this for directories where someone else could make a mistake and delete important files (like your home directory). Assuming your condor directory is called \"my condor_directory\", then you should do the following: cd my_condor_directory/ fs sa . nd_campus rlidwk fs sa . system:administrators rlidwka fs sa . system:authuser rlidwk You'll have to configure Condor so that any output files are stored in the specific directory to which you've granted write permissions. This will avoid possible problems in which condor does not have the correct permissions to launch jobs from your AFS space. [!WARNING] If you didn't do this the first time you logged in or created your Condor directory, this is fixable. See here <if-you-forgot>","title":"Setting up environment"},{"location":"resources/ndcms/#add-your-proxy-dn-and-notre-dame-username-your-username-on-earthcrcndedu","text":"1) Please add your proxy DN and ND username in the google doc below: https://docs.google.com/document/d/1piHU3tvAdPEXis-evx1mzOD8YwyQ59SbgeLUZR4U1cE/edit 2) Let us know that you've added your DN to the shared google doc file, please email ijohnso1@nd.edu and CC khurtado@nd.edu [!NOTE] If you would like to switch to a more modern shell, such as bash, you need to contact the CRC to have your shell changed in their database.**","title":"Add your proxy DN and Notre Dame username (your username on earth.crc.nd.edu)"},{"location":"resources/ndcms/#if-you-forgot-to-set-your-afs-permissions-at-first-login","text":"You can go back and set your AFS permissions recursively after the fact using the \"find\" command carefully: Starting in your home directory or one of your subdirectories, you can do the following to set read permissions for all subdirectories below. find . -type d -exec fs sa {} nd_campus rl \\; find . -type d -exec fs sa {} system:authuser rl \\; For the Condor directory and any of its subdirectories, you can use the following to set permissions correctly: cd my_condor_directory find . -type d -exec fs sa {} nd_campus rlidwk \\; find . -type d -exec fs sa {} system:administrators rlidwka \\; find . -type d -exec fs sa {} system:authuser rlidwk \\;","title":"If You Forgot to Set Your AFS Permissions at First Login"},{"location":"resources/ndcms/#phedex","text":"Back to Top of the Page <ndcms> . The CMS experiment is managing hundreds petabytes of data recorded by the detector and simulated physics events. Data are transferred to distributed CMS sites for storage, processing and analysis. PhEDEx (Physics Experiment Data Export) maintains the knowledge of data replicas locations and manages data transfers between the sites.","title":"PhEDEx"},{"location":"resources/ndcms/#requesting-a-phedex-transfer-to-the-notre-dame-tier-3-site-us_t3_notredame","text":"use DAS https://cmsweb.cern.ch/das/] to find the dataset you're interested in transferring In the search field, use this format to search for your favorite dataset: dataset dataset=/Tau/Run2011A-PromptReco-v4/AOD Click on \"Subscribe to PhEDEx\" Request a transfer to the US_T3_NotreDame site Under \"Destinations\" check the box next to T3_US_NotreDame At the bottom of the page, click \"Submit Request\", and click \"confirm\" on the following page. You will receive an email notification with a link to the status of your request. You will receive a confirmation email when your request is approved by the site admin.","title":"Requesting a PhEDEx Transfer to the Notre Dame Tier 3 Site (US_T3_NotreDame)"},{"location":"resources/ndcms/#monitoring-tier-3-components-and-jobs","text":"Back to Top of the Page <ndcms> . Below are a collection of links that can be used to try to monitor the health of the Tier 3 components. If you see unexpected behavior--fewer than normal running jobs, slower job I/O, failing file transfers, etc.--please check here for indications of what might be wrong. Note: Unless otherwise noted, the links below only work for those on campus or connected with a VPN.","title":"Monitoring Tier 3 components and Jobs"},{"location":"resources/ndcms/#htcondor-monitoring","text":"To check the general status of HTCondor within the CRC, you can view the HTCondor utilization matrix , for more information on HTCondor itself view the HTCondor <condor> page. The official documentation for conodr command line utilities can be found here .","title":"HTCondor Monitoring"},{"location":"resources/ndcms/#htcondor-job-monitoring","text":"To view the status of all jobs in the condor queue (or for a specific user), use the following command: condor_q username Note: if you have submitted jobs from outside ND, condor maps your username to \"uscms01\". If you have jobs in the \"Idle\" state, you can ask condor why they are not running, using the following command: condor_q -better {jobID} This command will give you a list of all the reasons a job can be rejected, and tell you how many of the 600 nodes are rejecting your job for that reason. It is possible to manually abort your jobs using the following command: condor_rm (jobID or username) However, it is recommended to cancel the jobs via CRAB, because CRAB will often get confused if its jobs suddenly disappear. To abort jobs using CRAB, use: crab -kill (job_range or \"all\") The memory restriction per node on the condor queue is set to 1 GB by default. This is measured against the virtual memory of the process, meaning once a job's image size (virtual memory) becomes larger than 1 GB it could be idled indefinitely. Many CMSSW jobs tend to accumulate much more virtual memory than physical memory (up to 2x in some cases). This causes jobs to be idled when they don't necessarily need to be. You can change the default 1 GB memory restriction through the command condor_qedit. If you choose to change this value, try to be reasonably sure that your jobs won't be using more than 1 GB of real memory as this could cause problems for the machine the job is running on (including possibly crashing it). In order to change the memory limit, do the following: 1) find the job requirements string for one of your jobs: > condor_q -l <condor_job_id> | grep Requirements 2) copy the string in quotes that comes after \"Requirements = \" in the output. The requirements string will have an element like ( ( TARGET.Memory * 1024 ) >= ImageSize ) Since we can't edit the value of 'TARGET.Memory', we'll have to change the proportionality factor...from 1024 to, say, 2048. 3) reset the Requirements string for a job or group of jobs, using the old requirements string with the new proportionality factor. > condor_qedit <user/condor_job_id> Requirements <new_req_string> Note that since the default string contains double quotation marks (\"\"), you'll need to surround your \\<new_req_string> with single qoutes (''). In addition, if you copy and paste the output from step one, you'll need to remove the space in \"> =\" to have instead \">=\". Remember that 'condor_qedit' will work on a specific job ID (xxx.y), a job cluster ID (xxx), or a username.","title":"HTCondor Job Monitoring"},{"location":"resources/ndcms/#storage","text":"Back to Top of the Page <ndcms> . Users have access to a variety of storage locations. The key is knowing the tradeoffs of each, so you can identify what's the right resource for you to use. If you don't have a directory in any of the spaces listed, ask for help at ndt3-list@nd.edu .","title":"Storage"},{"location":"resources/ndcms/#home-area","text":"Each user has a home directory on /afs/crc.nd.edu/user with 100GB of personal disk space that is backed up nightly. This is where you should keep software that you're developing, papers that you're writing, your thesis draft, etc. Basically, use this for anything that you would be very sad to have to recreate if it were accidentally deleted or lost due to hardware failure. To check the quota use fs lq (short for \u201cfileservice listquota\u201d)","title":"Home Area"},{"location":"resources/ndcms/#scratch-space","text":"Also users get 500GB of non-backed up space in /scratch365/<username> and non-backed up space for small files in /store/smallfiles . There is no quota on /store/smallfiles but the total space is 80 TB and must be shared by all users. In general, this storage is useful to use for temporary files of intermediate sizes. If you need reasonable access performance for multiple jobs to the files (e.g. you're going to run more than ~100 jobs reading or writing files in the batch system) then don't use /store/smallfiles as the performance degrades severely. In that case, either use /scratch365 or /hadoop/store/user (see below).","title":"Scratch Space"},{"location":"resources/ndcms/#accessing-the-scratch-space-from-grid-jobs","text":"Grid jobs running at ND (e.g: via CMS Connect) need to prepend /cms to the /scratch365 and /store/smallfiles directories ( /cms/scratch365/<username> and /cms/store/smallfiles ) due to standard requirements in the CMS Global Pool for singularity-enabled sites for non-hadoop storage spaces to be visible, but they point to the same storage location.","title":"Accessing the scratch space from grid jobs"},{"location":"resources/ndcms/#hadoop-space","text":"Hadoop is mounted at /hadoop/store/user . The Hadoop file system (hdfs) is a different sort of files system than most. Hadoop breaks your data up into blocks of ~128 MB and scatters two copies of each block across multiple physical disks. It does this for two reasons: The replication makes the system more resilient against hardware failures and it also provides better performance when many different jobs are reading or writing to the system. Like /store/smallfiles , Hadoop doesn't have per user quotas, and there is a lot of space available (at the time of this writing, 644 TB of raw space, but remember that every TB you store takes up ~2 TB of space because of replication). Hadoop is also the file system that is accessible with CMS/grid tools like gfal and XRootD. You should use Hadoop whenever you have very large datasets, when you need to access your data using gfal or XRootD, or when you will be accessing your data with many parallel running jobs (anything more than 100). There are some caveats: * Hadoop doesn't handle very small files well. If you write large numbers of files with sizes on the order of MB, don't use Hadoop. For files in that size range, use /store/smallfiles or /scratch365 * Hadoop doesn't provide posix access directly. This means that normally you can't use commands like ''ls'' or ''cp''. We use something called FUSE to provide posix access to Hadoop, but FUSE can be broken if you try to read too much data too quickly. So, when running many batch jobs, its better to access the data directly using hdfs commands, or to use a tool, like XRootD or Lobster that does this for you. If you're running jobs on data in /hadoop and they are getting stuck or having Input/Output errors, you've probably crashed FUSE on some of the nodes. If this happens, ask for help [ mailto:ndt3-list@nd.edu ndt3-list@nd.edu ]. * ROOT cannot write directly into /hadoop/store/user . If your job is producing ROOT output, write it first to local disk (every worker node has local disk for this purpose) and then at the end of the job, copy the output to /hadoop/store/user (possibly using gfal to avoid problems with FUSE!). Again, if you have questions, ask on ndt3-list@nd.edu .","title":"Hadoop Space"},{"location":"resources/ndcms/#lobster-working-storage","text":"Lobster doesn't do well working out of your AFS home directory. When you run Lobster jobs, you should tell Lobster to make your working area in /tmpscratch/users/<username> . Space is limited and there are no user quotas, so monitor carefully and clean up old files. We reserve the right to clean out this space if someone is using too much and not playing nice with others! [!NOTE] To Add you proxy DN and Notre Dame username, go to: https://wiki.crc.nd.edu/w/index.php/NDCMS_SettingUpEnvironment#Add_you_proxy_DN_and_Notre_Dame_username_.28your_username_on_earth.crc.nd.edu.29","title":"Lobster Working Storage"},{"location":"resources/ndcms/#storage-space-for-phedex","text":"All CMS data are stored using the /store convention, Therefore we only need to map: /+store/(.*) Translation rules for PFN to LFN (Physical File Name to Logical File Name): /hadoop/store ==> /store To see how much space is available in the hadoop /store area, you can type the following from earth: hadoop fs -df -h /store","title":"Storage space for PhEDEx"}]}